apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T12:25:06Z"
    generateName: elasticsearch-
    labels:
      app: elasticsearch
      controller-revision-hash: elasticsearch-9b5c5b58
      statefulset.kubernetes.io/pod-name: elasticsearch-0
    name: elasticsearch-0
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: elasticsearch
      uid: 9a0a213d-18ec-11ea-94c4-42010a9c0043
    resourceVersion: "510779"
    selfLink: /api/v1/namespaces/user111/pods/elasticsearch-0
    uid: 9a0da5f9-18ec-11ea-94c4-42010a9c0043
  spec:
    containers:
    - env:
      - name: cluster.name
        value: kubernetes-logging
      - name: node.name
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: discovery.seed_hosts
        value: elasticsearch-0.elasticsearch
      - name: cluster.initial_master_nodes
        value: elasticsearch-0
      - name: ES_JAVA_OPTS
        value: -Xms512m -Xmx512m
      image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
      imagePullPolicy: IfNotPresent
      name: elasticsearch
      ports:
      - containerPort: 9200
        protocol: TCP
      - containerPort: 9300
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/share/elasticsearch/data
        name: elastic-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: elasticsearch-0
    initContainers:
    - command:
      - sh
      - -c
      - chown -R 1000:1000 /usr/share/elasticsearch/data
      image: busybox
      imagePullPolicy: Always
      name: fix-permissions
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/share/elasticsearch/data
        name: elastic-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    - command:
      - sysctl
      - -w
      - vm.max_map_count=262144
      image: busybox
      imagePullPolicy: Always
      name: increase-vm-max-map
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    nodeName: gke-workshop-default-pool-9bcd7884-zhms
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: elasticsearch
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: elastic-data
      persistentVolumeClaim:
        claimName: elastic-data-elasticsearch-0
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:25:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:26:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:26:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:25:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://d3d3774ff16fc36f028ef757ec26ab2a248d199a16b9f1b1484ea110571c86e1
      image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
      imageID: docker-pullable://docker.elastic.co/elasticsearch/elasticsearch@sha256:25859ce3c9a4ac9b669411d2aa09d436ba34d39c23f0e3b6b058d8a7a0a8d36a
      lastState: {}
      name: elasticsearch
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T12:26:00Z"
    hostIP: 10.240.0.111
    initContainerStatuses:
    - containerID: docker://66085f36cf24153f9d337414c02dcd799b2d0147e69a2cc445265f5f7b66bda2
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:24fd20af232ca4ab5efbf1aeae7510252e2b60b15e9a78947467340607cd2ea2
      lastState: {}
      name: fix-permissions
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://66085f36cf24153f9d337414c02dcd799b2d0147e69a2cc445265f5f7b66bda2
          exitCode: 0
          finishedAt: "2019-12-07T12:25:31Z"
          reason: Completed
          startedAt: "2019-12-07T12:25:31Z"
    - containerID: docker://2da49cd4a9b1dc5449d8237d44c7e27642efb665de2ce5d8604df76a938c90af
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:24fd20af232ca4ab5efbf1aeae7510252e2b60b15e9a78947467340607cd2ea2
      lastState: {}
      name: increase-vm-max-map
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://2da49cd4a9b1dc5449d8237d44c7e27642efb665de2ce5d8604df76a938c90af
          exitCode: 0
          finishedAt: "2019-12-07T12:25:32Z"
          reason: Completed
          startedAt: "2019-12-07T12:25:32Z"
    phase: Running
    podIP: 10.24.7.9
    qosClass: Burstable
    startTime: "2019-12-07T12:25:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T11:53:43Z"
    generateName: grafana-
    labels:
      app: grafana
      controller-revision-hash: grafana-786cd7794c
      statefulset.kubernetes.io/pod-name: grafana-0
    name: grafana-0
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: grafana
      uid: 37f75133-18e8-11ea-94c4-42010a9c0043
    resourceVersion: "502652"
    selfLink: /api/v1/namespaces/user111/pods/grafana-0
    uid: 37f90fcc-18e8-11ea-94c4-42010a9c0043
  spec:
    containers:
    - image: grafana/grafana
      imagePullPolicy: Always
      name: grafana
      ports:
      - containerPort: 3000
        name: web
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: grafana-0
    nodeName: gke-workshop-default-pool-9bcd7884-mq8m
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T11:53:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T11:53:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T11:53:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T11:53:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6bd1bce905dbc0f402a3fba368fd1ba4b141880865d1d40b0611e51635602f55
      image: grafana/grafana:latest
      imageID: docker-pullable://grafana/grafana@sha256:98352c51c952d749e74899302eaffea749423ee379548a4d3fb829a74291783d
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T11:53:53Z"
    hostIP: 10.240.0.112
    phase: Running
    podIP: 10.24.10.6
    qosClass: BestEffort
    startTime: "2019-12-07T11:53:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T12:40:47Z"
    generateName: kafka-
    labels:
      app: kafka
      controller-revision-hash: kafka-56fc88685
      statefulset.kubernetes.io/pod-name: kafka-0
    name: kafka-0
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka
      uid: cabef6ec-18ee-11ea-94c4-42010a9c0043
    resourceVersion: "514548"
    selfLink: /api/v1/namespaces/user111/pods/kafka-0
    uid: cae726d2-18ee-11ea-94c4-42010a9c0043
  spec:
    affinity:
      podAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - zookeeper
            topologyKey: kubernetes.io/hostname
          weight: 1
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - kafka
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - sh
      - -c
      - 'exec kafka-server-start.sh /opt/kafka/config/server.properties --override
        broker.id=${HOSTNAME##*-} --override listeners=PLAINTEXT://:9093 --override
        zookeeper.connect=zookeeper:2181 --override log.dir=/var/lib/kafka --override
        auto.create.topics.enable=true --override auto.leader.rebalance.enable=true
        --override background.threads=10 --override compression.type=producer --override
        delete.topic.enable=false --override leader.imbalance.check.interval.seconds=300
        --override leader.imbalance.per.broker.percentage=10 --override log.flush.interval.messages=9223372036854775807
        --override log.flush.offset.checkpoint.interval.ms=60000 --override log.flush.scheduler.interval.ms=9223372036854775807
        --override log.retention.bytes=-1 --override log.retention.hours=168 --override
        log.roll.hours=168 --override log.roll.jitter.hours=0 --override log.segment.bytes=1073741824
        --override log.segment.delete.delay.ms=60000 --override message.max.bytes=1000012
        --override min.insync.replicas=1 --override num.io.threads=8 --override num.network.threads=3
        --override num.recovery.threads.per.data.dir=1 --override num.replica.fetchers=1
        --override offset.metadata.max.bytes=4096 --override offsets.commit.required.acks=-1
        --override offsets.commit.timeout.ms=5000 --override offsets.load.buffer.size=5242880
        --override offsets.retention.check.interval.ms=600000 --override offsets.retention.minutes=1440
        --override offsets.topic.compression.codec=0 --override offsets.topic.num.partitions=50
        --override offsets.topic.replication.factor=3 --override offsets.topic.segment.bytes=104857600
        --override queued.max.requests=500 --override quota.consumer.default=9223372036854775807
        --override quota.producer.default=9223372036854775807 --override replica.fetch.min.bytes=1
        --override replica.fetch.wait.max.ms=500 --override replica.high.watermark.checkpoint.interval.ms=5000
        --override replica.lag.time.max.ms=10000 --override replica.socket.receive.buffer.bytes=65536
        --override replica.socket.timeout.ms=30000 --override request.timeout.ms=30000
        --override socket.receive.buffer.bytes=102400 --override socket.request.max.bytes=104857600
        --override socket.send.buffer.bytes=102400 --override unclean.leader.election.enable=true
        --override zookeeper.session.timeout.ms=6000 --override zookeeper.set.acl=false
        --override broker.id.generation.enable=true --override connections.max.idle.ms=600000
        --override controlled.shutdown.enable=true --override controlled.shutdown.max.retries=3
        --override controlled.shutdown.retry.backoff.ms=5000 --override controller.socket.timeout.ms=30000
        --override default.replication.factor=1 --override fetch.purgatory.purge.interval.requests=1000
        --override group.max.session.timeout.ms=300000 --override group.min.session.timeout.ms=6000
        --override inter.broker.protocol.version=0.10.2-IV0 --override log.cleaner.backoff.ms=15000
        --override log.cleaner.dedupe.buffer.size=134217728 --override log.cleaner.delete.retention.ms=86400000
        --override log.cleaner.enable=true --override log.cleaner.io.buffer.load.factor=0.9
        --override log.cleaner.io.buffer.size=524288 --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308
        --override log.cleaner.min.cleanable.ratio=0.5 --override log.cleaner.min.compaction.lag.ms=0
        --override log.cleaner.threads=1 --override log.cleanup.policy=delete --override
        log.index.interval.bytes=4096 --override log.index.size.max.bytes=10485760
        --override log.message.timestamp.difference.max.ms=9223372036854775807 --override
        log.message.timestamp.type=CreateTime --override log.preallocate=false --override
        log.retention.check.interval.ms=300000 --override max.connections.per.ip=2147483647
        --override num.partitions=1 --override producer.purgatory.purge.interval.requests=1000
        --override replica.fetch.backoff.ms=1000 --override replica.fetch.max.bytes=1048576
        --override replica.fetch.response.max.bytes=10485760 --override reserved.broker.max.id=1000 '
      env:
      - name: KAFKA_HEAP_OPTS
        value: -Xmx512M -Xms512M
      - name: KAFKA_OPTS
        value: -Dlogging.level=INFO
      image: gcr.io/google_samples/k8skafka:v1
      imagePullPolicy: Always
      name: k8skafka
      ports:
      - containerPort: 9093
        name: server
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kafka
        name: datadir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-0
    nodeName: gke-workshop-default-pool-9bcd7884-zhms
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    subdomain: kafka-svc
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: datadir
      persistentVolumeClaim:
        claimName: datadir-kafka-0
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:40:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:40:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9e5e1bf22e80fd4e1afc733406e458f39a6801ef7949975bcd2096522f8e1f2f
      image: gcr.io/google_samples/k8skafka:v1
      imageID: docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
      lastState: {}
      name: k8skafka
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T12:40:57Z"
    hostIP: 10.240.0.111
    phase: Running
    podIP: 10.24.7.12
    qosClass: Burstable
    startTime: "2019-12-07T12:40:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T12:41:08Z"
    generateName: kafka-
    labels:
      app: kafka
      controller-revision-hash: kafka-56fc88685
      statefulset.kubernetes.io/pod-name: kafka-1
    name: kafka-1
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka
      uid: cabef6ec-18ee-11ea-94c4-42010a9c0043
    resourceVersion: "514731"
    selfLink: /api/v1/namespaces/user111/pods/kafka-1
    uid: d740f837-18ee-11ea-94c4-42010a9c0043
  spec:
    affinity:
      podAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - zookeeper
            topologyKey: kubernetes.io/hostname
          weight: 1
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - kafka
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - sh
      - -c
      - 'exec kafka-server-start.sh /opt/kafka/config/server.properties --override
        broker.id=${HOSTNAME##*-} --override listeners=PLAINTEXT://:9093 --override
        zookeeper.connect=zookeeper:2181 --override log.dir=/var/lib/kafka --override
        auto.create.topics.enable=true --override auto.leader.rebalance.enable=true
        --override background.threads=10 --override compression.type=producer --override
        delete.topic.enable=false --override leader.imbalance.check.interval.seconds=300
        --override leader.imbalance.per.broker.percentage=10 --override log.flush.interval.messages=9223372036854775807
        --override log.flush.offset.checkpoint.interval.ms=60000 --override log.flush.scheduler.interval.ms=9223372036854775807
        --override log.retention.bytes=-1 --override log.retention.hours=168 --override
        log.roll.hours=168 --override log.roll.jitter.hours=0 --override log.segment.bytes=1073741824
        --override log.segment.delete.delay.ms=60000 --override message.max.bytes=1000012
        --override min.insync.replicas=1 --override num.io.threads=8 --override num.network.threads=3
        --override num.recovery.threads.per.data.dir=1 --override num.replica.fetchers=1
        --override offset.metadata.max.bytes=4096 --override offsets.commit.required.acks=-1
        --override offsets.commit.timeout.ms=5000 --override offsets.load.buffer.size=5242880
        --override offsets.retention.check.interval.ms=600000 --override offsets.retention.minutes=1440
        --override offsets.topic.compression.codec=0 --override offsets.topic.num.partitions=50
        --override offsets.topic.replication.factor=3 --override offsets.topic.segment.bytes=104857600
        --override queued.max.requests=500 --override quota.consumer.default=9223372036854775807
        --override quota.producer.default=9223372036854775807 --override replica.fetch.min.bytes=1
        --override replica.fetch.wait.max.ms=500 --override replica.high.watermark.checkpoint.interval.ms=5000
        --override replica.lag.time.max.ms=10000 --override replica.socket.receive.buffer.bytes=65536
        --override replica.socket.timeout.ms=30000 --override request.timeout.ms=30000
        --override socket.receive.buffer.bytes=102400 --override socket.request.max.bytes=104857600
        --override socket.send.buffer.bytes=102400 --override unclean.leader.election.enable=true
        --override zookeeper.session.timeout.ms=6000 --override zookeeper.set.acl=false
        --override broker.id.generation.enable=true --override connections.max.idle.ms=600000
        --override controlled.shutdown.enable=true --override controlled.shutdown.max.retries=3
        --override controlled.shutdown.retry.backoff.ms=5000 --override controller.socket.timeout.ms=30000
        --override default.replication.factor=1 --override fetch.purgatory.purge.interval.requests=1000
        --override group.max.session.timeout.ms=300000 --override group.min.session.timeout.ms=6000
        --override inter.broker.protocol.version=0.10.2-IV0 --override log.cleaner.backoff.ms=15000
        --override log.cleaner.dedupe.buffer.size=134217728 --override log.cleaner.delete.retention.ms=86400000
        --override log.cleaner.enable=true --override log.cleaner.io.buffer.load.factor=0.9
        --override log.cleaner.io.buffer.size=524288 --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308
        --override log.cleaner.min.cleanable.ratio=0.5 --override log.cleaner.min.compaction.lag.ms=0
        --override log.cleaner.threads=1 --override log.cleanup.policy=delete --override
        log.index.interval.bytes=4096 --override log.index.size.max.bytes=10485760
        --override log.message.timestamp.difference.max.ms=9223372036854775807 --override
        log.message.timestamp.type=CreateTime --override log.preallocate=false --override
        log.retention.check.interval.ms=300000 --override max.connections.per.ip=2147483647
        --override num.partitions=1 --override producer.purgatory.purge.interval.requests=1000
        --override replica.fetch.backoff.ms=1000 --override replica.fetch.max.bytes=1048576
        --override replica.fetch.response.max.bytes=10485760 --override reserved.broker.max.id=1000 '
      env:
      - name: KAFKA_HEAP_OPTS
        value: -Xmx512M -Xms512M
      - name: KAFKA_OPTS
        value: -Dlogging.level=INFO
      image: gcr.io/google_samples/k8skafka:v1
      imagePullPolicy: Always
      name: k8skafka
      ports:
      - containerPort: 9093
        name: server
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kafka
        name: datadir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-1
    nodeName: gke-workshop-default-pool-9bcd7884-2xvj
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    subdomain: kafka-svc
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: datadir
      persistentVolumeClaim:
        claimName: datadir-kafka-1
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://32332d233d5fbd46e3cafcc05b7987d51e08819c5ca518889a3fb9e757725f9c
      image: gcr.io/google_samples/k8skafka:v1
      imageID: docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
      lastState: {}
      name: k8skafka
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T12:41:45Z"
    hostIP: 10.240.0.113
    phase: Running
    podIP: 10.24.9.12
    qosClass: Burstable
    startTime: "2019-12-07T12:41:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T12:41:49Z"
    generateName: kafka-
    labels:
      app: kafka
      controller-revision-hash: kafka-56fc88685
      statefulset.kubernetes.io/pod-name: kafka-2
    name: kafka-2
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: kafka
      uid: cabef6ec-18ee-11ea-94c4-42010a9c0043
    resourceVersion: "514942"
    selfLink: /api/v1/namespaces/user111/pods/kafka-2
    uid: f03bd65d-18ee-11ea-94c4-42010a9c0043
  spec:
    affinity:
      podAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - zookeeper
            topologyKey: kubernetes.io/hostname
          weight: 1
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - kafka
          topologyKey: kubernetes.io/hostname
    containers:
    - command:
      - sh
      - -c
      - 'exec kafka-server-start.sh /opt/kafka/config/server.properties --override
        broker.id=${HOSTNAME##*-} --override listeners=PLAINTEXT://:9093 --override
        zookeeper.connect=zookeeper:2181 --override log.dir=/var/lib/kafka --override
        auto.create.topics.enable=true --override auto.leader.rebalance.enable=true
        --override background.threads=10 --override compression.type=producer --override
        delete.topic.enable=false --override leader.imbalance.check.interval.seconds=300
        --override leader.imbalance.per.broker.percentage=10 --override log.flush.interval.messages=9223372036854775807
        --override log.flush.offset.checkpoint.interval.ms=60000 --override log.flush.scheduler.interval.ms=9223372036854775807
        --override log.retention.bytes=-1 --override log.retention.hours=168 --override
        log.roll.hours=168 --override log.roll.jitter.hours=0 --override log.segment.bytes=1073741824
        --override log.segment.delete.delay.ms=60000 --override message.max.bytes=1000012
        --override min.insync.replicas=1 --override num.io.threads=8 --override num.network.threads=3
        --override num.recovery.threads.per.data.dir=1 --override num.replica.fetchers=1
        --override offset.metadata.max.bytes=4096 --override offsets.commit.required.acks=-1
        --override offsets.commit.timeout.ms=5000 --override offsets.load.buffer.size=5242880
        --override offsets.retention.check.interval.ms=600000 --override offsets.retention.minutes=1440
        --override offsets.topic.compression.codec=0 --override offsets.topic.num.partitions=50
        --override offsets.topic.replication.factor=3 --override offsets.topic.segment.bytes=104857600
        --override queued.max.requests=500 --override quota.consumer.default=9223372036854775807
        --override quota.producer.default=9223372036854775807 --override replica.fetch.min.bytes=1
        --override replica.fetch.wait.max.ms=500 --override replica.high.watermark.checkpoint.interval.ms=5000
        --override replica.lag.time.max.ms=10000 --override replica.socket.receive.buffer.bytes=65536
        --override replica.socket.timeout.ms=30000 --override request.timeout.ms=30000
        --override socket.receive.buffer.bytes=102400 --override socket.request.max.bytes=104857600
        --override socket.send.buffer.bytes=102400 --override unclean.leader.election.enable=true
        --override zookeeper.session.timeout.ms=6000 --override zookeeper.set.acl=false
        --override broker.id.generation.enable=true --override connections.max.idle.ms=600000
        --override controlled.shutdown.enable=true --override controlled.shutdown.max.retries=3
        --override controlled.shutdown.retry.backoff.ms=5000 --override controller.socket.timeout.ms=30000
        --override default.replication.factor=1 --override fetch.purgatory.purge.interval.requests=1000
        --override group.max.session.timeout.ms=300000 --override group.min.session.timeout.ms=6000
        --override inter.broker.protocol.version=0.10.2-IV0 --override log.cleaner.backoff.ms=15000
        --override log.cleaner.dedupe.buffer.size=134217728 --override log.cleaner.delete.retention.ms=86400000
        --override log.cleaner.enable=true --override log.cleaner.io.buffer.load.factor=0.9
        --override log.cleaner.io.buffer.size=524288 --override log.cleaner.io.max.bytes.per.second=1.7976931348623157E308
        --override log.cleaner.min.cleanable.ratio=0.5 --override log.cleaner.min.compaction.lag.ms=0
        --override log.cleaner.threads=1 --override log.cleanup.policy=delete --override
        log.index.interval.bytes=4096 --override log.index.size.max.bytes=10485760
        --override log.message.timestamp.difference.max.ms=9223372036854775807 --override
        log.message.timestamp.type=CreateTime --override log.preallocate=false --override
        log.retention.check.interval.ms=300000 --override max.connections.per.ip=2147483647
        --override num.partitions=1 --override producer.purgatory.purge.interval.requests=1000
        --override replica.fetch.backoff.ms=1000 --override replica.fetch.max.bytes=1048576
        --override replica.fetch.response.max.bytes=10485760 --override reserved.broker.max.id=1000 '
      env:
      - name: KAFKA_HEAP_OPTS
        value: -Xmx512M -Xms512M
      - name: KAFKA_OPTS
        value: -Dlogging.level=INFO
      image: gcr.io/google_samples/k8skafka:v1
      imagePullPolicy: Always
      name: k8skafka
      ports:
      - containerPort: 9093
        name: server
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - sh
          - -c
          - /opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9093
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        requests:
          cpu: 500m
          memory: 1Gi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kafka
        name: datadir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: kafka-2
    nodeName: gke-workshop-default-pool-9bcd7884-47sd
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    subdomain: kafka-svc
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: datadir
      persistentVolumeClaim:
        claimName: datadir-kafka-2
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:42:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:42:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:41:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://957de37335c9be0332cf0441bab1ebb9121d9f63fd8434fd711f81f4651196a1
      image: gcr.io/google_samples/k8skafka:v1
      imageID: docker-pullable://gcr.io/google_samples/k8skafka@sha256:1be8f40245992b94196c998d42a27da3840104c41eb78b8a389276a2c5d3b96f
      lastState: {}
      name: k8skafka
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T12:42:26Z"
    hostIP: 10.240.0.101
    phase: Running
    podIP: 10.24.8.7
    qosClass: Burstable
    startTime: "2019-12-07T12:41:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T13:39:23Z"
    generateName: kibana-598dc944d9-
    labels:
      app: kibana
      pod-template-hash: 598dc944d9
    name: kibana-598dc944d9-fl4m4
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kibana-598dc944d9
      uid: fae3c195-18f6-11ea-94c4-42010a9c0043
    resourceVersion: "529435"
    selfLink: /api/v1/namespaces/user111/pods/kibana-598dc944d9-fl4m4
    uid: fae4c0fe-18f6-11ea-94c4-42010a9c0043
  spec:
    containers:
    - env:
      - name: ELASTICSEARCH_URL
        value: http://elasticsearch:9200
      image: docker.elastic.co/kibana/kibana:7.2.0
      imagePullPolicy: IfNotPresent
      name: kibana
      ports:
      - containerPort: 5601
        protocol: TCP
      resources:
        limits:
          cpu: "1"
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-workshop-default-pool-9bcd7884-mq8m
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T13:39:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T13:40:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T13:40:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T13:39:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://be4af57c5281d6f87d94cfc9567303a3e418b9e2e3336f42ec7f64df7ecad9db
      image: docker.elastic.co/kibana/kibana:7.2.0
      imageID: docker-pullable://docker.elastic.co/kibana/kibana@sha256:1579f95db4242327cf0637a79cbbe095fcae11a772e324482adf4fe0e0b3ac82
      lastState: {}
      name: kibana
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T13:40:24Z"
    hostIP: 10.240.0.112
    phase: Running
    podIP: 10.24.10.15
    qosClass: Burstable
    startTime: "2019-12-07T13:39:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T15:16:59Z"
    generateName: logstash-
    labels:
      app: logstash
      controller-revision-hash: logstash-5bbc5785d6
      statefulset.kubernetes.io/pod-name: logstash-0
    name: logstash-0
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: logstash
      uid: 63a497b1-18f5-11ea-94c4-42010a9c0043
    resourceVersion: "559104"
    selfLink: /api/v1/namespaces/user111/pods/logstash-0
    uid: 9d5e5f3a-1904-11ea-94c4-42010a9c0043
  spec:
    containers:
    - env:
      - name: HTTP_HOST
        value: 0.0.0.0
      - name: HTTP_PORT
        value: "9600"
      - name: ELASTICSEARCH_HOST
        value: elasticsearch
      - name: ELASTICSEARCH_PORT
        value: "9200"
      - name: LS_JAVA_OPTS
        value: -Xmx1g -Xms1g
      - name: CONFIG_RELOAD_AUTOMATIC
        value: "true"
      - name: PATH_CONFIG
        value: /usr/share/logstash/pipeline
      - name: PATH_DATA
        value: /usr/share/logstash/data
      - name: QUEUE_CHECKPOINT_WRITES
        value: "1"
      - name: QUEUE_DRAIN
        value: "true"
      - name: QUEUE_MAX_BYTES
        value: 1gb
      - name: QUEUE_TYPE
        value: persisted
      image: docker.elastic.co/logstash/logstash-oss:7.1.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: monitor
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: logstash
      ports:
      - containerPort: 9600
        name: monitor
        protocol: TCP
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 5044
        name: beats
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: monitor
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /usr/share/logstash/data
        name: data
      - mountPath: /usr/share/logstash/pipeline
        name: pipeline
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    - args:
      - |-
        sleep 60; exec /logstash_exporter
          --logstash.endpoint=http://localhost:9600
          --web.listen-address=:9198
      command:
      - /bin/sh
      - -c
      env:
      - name: HTTP_HOST
        value: 0.0.0.0
      - name: HTTP_PORT
        value: "9600"
      - name: ELASTICSEARCH_HOST
        value: elasticsearch
      - name: ELASTICSEARCH_PORT
        value: "9200"
      - name: LS_JAVA_OPTS
        value: -Xmx1g -Xms1g
      - name: CONFIG_RELOAD_AUTOMATIC
        value: "true"
      - name: PATH_CONFIG
        value: /usr/share/logstash/pipeline
      - name: PATH_DATA
        value: /usr/share/logstash/data
      - name: QUEUE_CHECKPOINT_WRITES
        value: "1"
      - name: QUEUE_DRAIN
        value: "true"
      - name: QUEUE_MAX_BYTES
        value: 1gb
      - name: QUEUE_TYPE
        value: persisted
      image: bonniernews/logstash_exporter:v0.1.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          path: /metrics
          port: ls-exporter
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 60
      name: logstash-exporter
      ports:
      - containerPort: 9198
        name: ls-exporter
        protocol: TCP
      readinessProbe:
        failureThreshold: 8
        httpGet:
          path: /metrics
          port: ls-exporter
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 60
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: logstash-0
    nodeName: gke-workshop-default-pool-9bcd7884-1jqc
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000
      runAsUser: 1000
    serviceAccount: default
    serviceAccountName: default
    subdomain: logstash
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: data
      persistentVolumeClaim:
        claimName: data-logstash-0
    - configMap:
        defaultMode: 420
        name: logstash-pipeline
      name: pipeline
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:16:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:18:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:18:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:16:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ed04fb39efbc6027e1226e189624af51dde5479b8285dc72cc1806c029d8f45d
      image: docker.elastic.co/logstash/logstash-oss:7.1.1
      imageID: docker-pullable://docker.elastic.co/logstash/logstash-oss@sha256:a5e216df5d2bc6f7262f730d96586144ac6ebe73b6e48e50a1e919a7ea2b9c1e
      lastState: {}
      name: logstash
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T15:17:10Z"
    - containerID: docker://ba973c2589d8545c681a7464cf44fd2a1a43dbd50c042d75ce61db2f2cf355a4
      image: bonniernews/logstash_exporter:v0.1.2
      imageID: docker-pullable://bonniernews/logstash_exporter@sha256:7efc532990d14bc50e80aea8c3dc2c9a7c32c5ad47535687f085fb1740ee1b36
      lastState: {}
      name: logstash-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T15:17:10Z"
    hostIP: 10.240.0.103
    phase: Running
    podIP: 10.24.11.31
    qosClass: BestEffort
    startTime: "2019-12-07T15:16:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T15:34:50Z"
    generateName: prometheus-
    labels:
      app: prometheus
      app.kubernetes.io/component: prometheus
      controller-revision-hash: prometheus-7c5cd845db
      statefulset.kubernetes.io/pod-name: prometheus-0
    name: prometheus-0
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus
      uid: 7dec8de1-1906-11ea-94c4-42010a9c0043
    resourceVersion: "563409"
    selfLink: /api/v1/namespaces/user111/pods/prometheus-0
    uid: 1b871562-1907-11ea-94c4-42010a9c0043
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - prometheus
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config.file=/etc/config/prometheus.yaml
      - --storage.tsdb.path=/data
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
      - --web.enable-lifecycle
      image: prom/prometheus
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: prometheus-server
      ports:
      - containerPort: 9090
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9090
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        requests:
          cpu: 200m
          memory: 1000Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: config-volume
      - mountPath: /data
        name: prometheus-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-0
    initContainers:
    - command:
      - chown
      - -R
      - 65534:65534
      - /data
      image: busybox
      imagePullPolicy: Always
      name: init-chown-data
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: prometheus-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    nodeName: gke-workshop-default-pool-9bcd7884-mq8m
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: prometheus
    terminationGracePeriodSeconds: 300
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-data
      persistentVolumeClaim:
        claimName: prometheus-data-prometheus-0
    - configMap:
        defaultMode: 420
        name: prometheus-config
      name: config-volume
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:35:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:35:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:35:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:34:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a37bd2142f78f82e2e22689aedd2ad11d61bcc0852bfc46d0e08fe6743a3faf7
      image: prom/prometheus:latest
      imageID: docker-pullable://prom/prometheus@sha256:907e20b3b0f8b0a76a33c088fe9827e8edc180e874bd2173c27089eade63d8b8
      lastState: {}
      name: prometheus-server
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T15:35:01Z"
    hostIP: 10.240.0.112
    initContainerStatuses:
    - containerID: docker://5ee76f8029f9e531e0997e116cbb8fd5398bcf24df3865c2411003f564becfbd
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:24fd20af232ca4ab5efbf1aeae7510252e2b60b15e9a78947467340607cd2ea2
      lastState: {}
      name: init-chown-data
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: docker://5ee76f8029f9e531e0997e116cbb8fd5398bcf24df3865c2411003f564becfbd
          exitCode: 0
          finishedAt: "2019-12-07T15:35:01Z"
          reason: Completed
          startedAt: "2019-12-07T15:35:01Z"
    phase: Running
    podIP: 10.24.10.25
    qosClass: Burstable
    startTime: "2019-12-07T15:34:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"random-logger","namespace":"user111"},"spec":{"containers":[{"args":["/bin/sh","-c","i=0; while true; do\n  echo \"$i: $(date)\" \u003e\u003e /var/log/1.log;\n  echo \"$(date) INFO $i\" \u003e\u003e /var/log/2.log;\n  i=$((i+1));\n  sleep 1;\ndone\n"],"image":"busybox","name":"count","volumeMounts":[{"mountPath":"/var/log","name":"varlog"}]},{"args":["/bin/sh","-c","tail -n+1 -f /var/log/1.log"],"image":"busybox","name":"count-log-1","volumeMounts":[{"mountPath":"/var/log","name":"varlog"}]},{"image":"docker.elastic.co/beats/filebeat:5.4.0","name":"filebeat","volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/usr/share/filebeat/filebeat.yml","name":"filebeat-config","subPath":"filebeat.yml"}]}],"volumes":[{"configMap":{"items":[{"key":"filebeat.yml","path":"filebeat.yml"}],"name":"filebeat-config"},"name":"filebeat-config"},{"emptyDir":{},"name":"varlog"}]}}
    creationTimestamp: "2019-12-07T15:44:15Z"
    name: random-logger
    namespace: user111
    resourceVersion: "565478"
    selfLink: /api/v1/namespaces/user111/pods/random-logger
    uid: 6c5a578f-1908-11ea-94c4-42010a9c0043
  spec:
    containers:
    - args:
      - /bin/sh
      - -c
      - |
        i=0; while true; do
          echo "$i: $(date)" >> /var/log/1.log;
          echo "$(date) INFO $i" >> /var/log/2.log;
          i=$((i+1));
          sleep 1;
        done
      image: busybox
      imagePullPolicy: Always
      name: count
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    - args:
      - /bin/sh
      - -c
      - tail -n+1 -f /var/log/1.log
      image: busybox
      imagePullPolicy: Always
      name: count-log-1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    - image: docker.elastic.co/beats/filebeat:5.4.0
      imagePullPolicy: IfNotPresent
      name: filebeat
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /usr/share/filebeat/filebeat.yml
        name: filebeat-config
        subPath: filebeat.yml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-workshop-default-pool-9bcd7884-mq8m
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: filebeat.yml
          path: filebeat.yml
        name: filebeat-config
      name: filebeat-config
    - emptyDir: {}
      name: varlog
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:44:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:44:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:44:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T15:44:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://69816b682ed489d277b28d0b5824e3809f1a13959ccb4594bf490627ea5a9e5f
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:24fd20af232ca4ab5efbf1aeae7510252e2b60b15e9a78947467340607cd2ea2
      lastState: {}
      name: count
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T15:44:17Z"
    - containerID: docker://e6721554d72811ceab0f16c3e49457d32eb8bbe28921003822c05a8c4508d546
      image: busybox:latest
      imageID: docker-pullable://busybox@sha256:24fd20af232ca4ab5efbf1aeae7510252e2b60b15e9a78947467340607cd2ea2
      lastState: {}
      name: count-log-1
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T15:44:17Z"
    - containerID: docker://44db3e04aa41a8a5bb29b951a081e5f2d7f5b26c013b620296242a6d9d7e65c4
      image: docker.elastic.co/beats/filebeat:5.4.0
      imageID: docker-pullable://docker.elastic.co/beats/filebeat@sha256:cbe7e0e5e15ad377ce1a9255b68ca10c662d51b4edcf720f615ffdd88eca83ec
      lastState: {}
      name: filebeat
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T15:44:18Z"
    hostIP: 10.240.0.112
    phase: Running
    podIP: 10.24.10.26
    qosClass: BestEffort
    startTime: "2019-12-07T15:44:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-12-07T12:40:19Z"
    generateName: zookeeper-5fbd975dc-
    labels:
      app: zookeeper
      pod-template-hash: 5fbd975dc
    name: zookeeper-5fbd975dc-x59dv
    namespace: user111
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: zookeeper-5fbd975dc
      uid: ba2e9f60-18ee-11ea-94c4-42010a9c0043
    resourceVersion: "514321"
    selfLink: /api/v1/namespaces/user111/pods/zookeeper-5fbd975dc-x59dv
    uid: ba30211c-18ee-11ea-94c4-42010a9c0043
  spec:
    containers:
    - env:
      - name: KAFKA_HEAP_OPTS
        value: -Xms32M -Xmx32M -verbose:gc
      - name: ZOOKEEPER_CLIENT_PORT
        value: "2181"
      - name: ZOOKEEPER_SERVERS
        value: zookeeper:4182:5181
      - name: ZOOKEEPER_SERVER_ID
        value: "1"
      image: confluentinc/cp-zookeeper:5.2.2
      imagePullPolicy: IfNotPresent
      name: zookeeper
      ports:
      - containerPort: 2181
        name: zk-port
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-jxwts
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: gke-workshop-default-pool-9bcd7884-zhms
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-jxwts
      secret:
        defaultMode: 420
        secretName: default-token-jxwts
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:40:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:40:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:40:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-07T12:40:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://50994f173ed26d901aa11add2074e7844c035d6cdc713e4678fcda2ab5ba8961
      image: confluentinc/cp-zookeeper:5.2.2
      imageID: docker-pullable://confluentinc/cp-zookeeper@sha256:d18d1a37ca29313fb988fdba1a0072314494d9596fa39f85b44ce815b48f9ea9
      lastState: {}
      name: zookeeper
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-07T12:40:20Z"
    hostIP: 10.240.0.111
    phase: Running
    podIP: 10.24.7.11
    qosClass: BestEffort
    startTime: "2019-12-07T12:40:19Z"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
